# -*- coding: utf-8 -*-
try:
    # Fix UTF8 output issues on Windows console.
    # Does nothing if package is not installed
    from win_unicode_console import enable
    enable()
except ImportError:
    pass

def utfReverse(s):
    # CAVEAT: this will mess up characters that are
    # more than 2 bytes long in utf 16
    u = s.decode("utf-8")
    return u[::-1].encode("utf-8")

######################################################################

import requests
import json

def callApi(url, data, tokenKey):    
    headers = {
        'Content-Type': "application/json",
        'Authorization': "Bearer " + tokenKey,
        'Cache-Control': "no-cache"
    }
    response = requests.request("POST", url, data=data.encode("utf-8"), headers=headers)
    return response.text    # return utfReverse(response.text.encode("utf-8"))
    
##################### Get Token by Api Key ##########################
baseUrl = "http://api.text-mining.ir/api/"
url = baseUrl + "Token/GetToken"
querystring = {"apikey":"YOUR_API_KEY"}
response = requests.request("GET", url, params=querystring)
data = json.loads(response.text)
tokenKey = data['token']

######################## Call Normalizer ############################
url =  baseUrl + "PreProcessing/NormalizePersianWord"
payload = u"{\"text\":\"ูููู ุงฺฏููุฑ ุฏฺชูููููู ูููฺชููุซ ุฑู ููููููุณ ฺชูููููููู ฺชูููููุง ูููุชููู ฺูููููุฏููู ุตูููููุญููู ุฌููุงุจููู ุฌููุง ูููููุดููู ู ุฏููฺฏููู ููููููููุดููู ูููููููููุฏ ฺชููุฏูู ุขููู ุชูููููุงูุช ููู ุดูุฏ ุจููุงููุฏ ฺูู ฺชููููููููุ.\", \"refineSeparatedAffix\":true}"
print(callApi(url, payload, tokenKey))
# result: ูู ุงฺฏุฑ ุฏฺฉูู ูฺฉุซ ุฑู ููุณ ฺฉูู ฺฉูุง ูุชู ฺูุฏู ุตูุญู ุฌุงุจู ุฌุง ูุดู ู ุฏฺฏู ููุดู ูููุฏ ฺฉุฏูู ุขู ุชูุงูุช ูโุดูุฏ ุจุงุฏ ฺ ฺฉููุ.

##################### Call Sentence Splitter ########################
url =  baseUrl + "PreProcessing/SentenceSplitter"
payload = u'''{\"text\": \"ูู ุจุง ุฏูุณุชู ุจู ูุฏุฑุณู ู ุฑูุชู ู ุฏุฑ ุขูุฌุง ูุดุบูู ุจู ุชุญุตู ุจูุฏู. ุณูพุณ ุจู ุฏุงูุดฺฏุงู ุฑุงู ุงูุชู\",
    \"checkSlang\": true, 
    \"normalize\": true, 
    \"normalizerParams\": {
        \"text\": \"don't care\",
        \"RefineQuotationPunc \": false
    },
    \"complexSentence\": true
}'''
print(callApi(url, payload, tokenKey))
# resuilt: ["ูู ุจุง ุฏูุณุชู ุจู ูุฏุฑุณู ูโุฑูุชู","ู ุฏุฑ ุขูุฌุง ูุดุบูู ุจู ุชุญุตู ุจูุฏู .","ุณูพุณ ุจู ุฏุงูุดฺฏุงู ุฑุงู ุงูุชู"

######################## Call Tokenizer ############################
url =  baseUrl + "PreProcessing/Tokenize"
payload = u"\"ูู ุจุง ุฏุงูุดุฌูุงู ุฏฺฏุฑ ุจุฑุฎูุฑุฏ ฺฉุฑุฏู\""
print(callApi(url, payload, tokenKey))
# rsult: ["ูู","ุจุง","ุฏุงูุดุฌูุงู","ุฏฺฏุฑ","ุจุฑุฎูุฑุฏ","ฺฉุฑุฏู"]

url =  baseUrl + "PreProcessing/TokenizeWithType"
payload = u"\"ุงุฎุจุงุฑ 20:30 ููุฑุฎ 1398/2/22 ุงุนูุงู ฺฉุฑุฏ ุดุฑฺฉุช T.E.T ูุจูุบ 200.57 ูููู ุงุฑุฒุด ุฏุงุฑุฏ!!!  ๐ @Khabar_Alaki -- email: hi@text-mining.ir\""
print(callApi(url, payload, tokenKey))
#result: [{"key":"ุงุฎุจุงุฑ","value":"Word"},{"key":"20:30 ","value":"DateTime"},{"key":"ููุฑุฎ","value":"Word"},{"key":"1398/2/22","value":"DateTime"},{"key":"ุงุนูุงู","value":"Word"},{"key":"ฺฉุฑุฏ","value":"Word"},{"key":"ุดุฑฺฉุช","value":"Word"},{"key":"T.E.T","value":"Abbreviation"},{"key":"ูุจูุบ","value":"Word"},{"key":"200.57","value":"Number"},{"key":"ูููู","value":"Word"},{"key":"ุงุฑุฒุด","value":"Word"},{"key":"ุฏุงุฑุฏ","value":"Word"},{"key":"!!!","value":"Separator"},{"key":"๐","value":"Emoji"},{"key":"@Khabar_Alaki","value":"SocialId"},{"key":"--","value":"Separator"},{"key":"email","value":"Word"},{"key":":","value":"Separator"},{"key":"hi@text-mining.ir","value":"Email"}]

############# Call Sentence Splitter and Tokenizer #################
url =  baseUrl + "PreProcessing/SentenceSplitterAndTokenize"
payload = u'''{\"text\": \"ูู ุจุง ุฏูุณุชู ุจู ูุฏุฑุณู ู ุฑูุชู ู ุฏุฑ ุขูุฌุง ูุดุบูู ุจู ุชุญุตู ุจูุฏู. ุณูพุณ ุจู ุฏุงูุดฺฏุงู ุฑุงู ุงูุชู\",
    \"checkSlang\": true, 
    \"normalize\": true, 
    \"normalizerParams\": {
        \"text\": \"don't care\",
        \"replaceWildChar\": true,
        \"replaceDigit\": true,
        \"refineSeparatedAffix\": true,
        \"refineQuotationPunc\": false
    },
    \"complexSentence\": true
}'''
print(callApi(url, payload, tokenKey))
# result: [["ูู","ุจุง","ุฏูุณุชู","ุจู","ูุฏุฑุณู","ูโุฑูุชู"],["ู","ุฏุฑ","ุขูุฌุง","ูุดุบูู","ุจู","ุชุญุตู","ุจูุฏู","."],["ุณูพุณ","ุจู","ุฏุงูุดฺฏุงู","ุฑุงู","ุงูุชู"]]

########################## Call Stemmer ##########################
url =  baseUrl + "Stemmer/LemmatizeText2Text"
payload = u'"ูู ุจุง ุฏุงูุดุฌูุงู ุฏฺฏุฑ ุจุฑุฎูุฑุฏ ฺฉุฑุฏู. ุณูพุณ ุจู ุขููุง ฺฏูุชู\nูู ุจุง ุดูุง ฺฉุงุฑูุง ุฒุงุฏ ุฏุงุฑู"'
print(callApi(url, payload, tokenKey))
''' result: 
ูู ุจุง ุฏุงูุดุฌู ุฏฺฏุฑ ุจุฑ ุฎูุฑุฏ ฺฉุฑุฏ. ุณูพุณ ุจู ุขู ฺฏูุช
ูู ุจุง ุดูุง ฺฉุงุฑ ุฒุงุฏ ุฏุงุดุช
'''

url =  baseUrl + "Stemmer/LemmatizePhrase2Phrase"
payload = u'{"phrases": [{ "word": "ุฏุฑุงููุฑุฏุงู" }, { "word": "ูุฑุดุชฺฏุงู" }], "checkSlang": false}'
print(callApi(url, payload, tokenKey))
# result: [{"wordComment":"","simplePos":"","rootWords":["ุฏุฑุงููุฑุฏ","ุฏุฑุง"],"verbInformation":null,"sentenceNumber":0,"wordNumberInSentence":0,"startCharIndex":0,"word":"ุฏุฑุงููุฑุฏุงู","tags":{},"firstRoot":"ุฏุฑุงููุฑุฏ","wordCount":1,"length":11,"isVerb":false,"isPunc":false},{"wordComment":"","simplePos":"","rootWords":["ูุฑุดุชู"],"verbInformation":null,"sentenceNumber":0,"wordNumberInSentence":0,"startCharIndex":0,"word":"ูุฑุดุชฺฏุงู","tags":{},"firstRoot":"ูุฑุดุชู","wordCount":1,"length":7,"isVerb":false,"isPunc":false}]

url =  baseUrl + "Stemmer/LemmatizeText2Phrase"
payload = u'{"text": "ุฏุงูุดุฌูุงู ุฒุงุฏ ุจู ูุฏุงุฑุณ ุงุณุชุนุฏุงุฏูุง ุฏุฑุฎุดุงู ุฑุงู ูพุฏุง ูุฎูุงููุฏ ฺฉุฑุฏ ฺฉู ุจุง ูุดฺฉูุงุช ุจุนุฏ ููุงุฌู ุดููุฏ.", "checkSlang": false}'
result = json.loads(callApi(url, payload, tokenKey))
for phrase in result:
    print("("+phrase['word']+":"+phrase['firstRoot']+") ")
''' result:
(ุฏุงูุดุฌูุงู:ุฏุงูุดุฌู)
(ุฒุงุฏ:ุฒุงุฏ)
(ุจู:ุจู)
(ูุฏุงุฑุณ:ูุฏุฑุณู)
(ุงุณุชุนุฏุงุฏูุง:ุงุณุชุนุฏุงุฏ)
(ุฏุฑุฎุดุงู:ุฏุฑุฎุดุงู)
(ุฑุงู:ุฑุงู)
(ูพุฏุง:ูพุฏุง)
(ูุฎูุงููุฏ ฺฉุฑุฏ:ูฺฉุฑุฏ)
(ฺฉู:ฺฉู)
(ุจุง:ุจุง)
(ูุดฺฉูุงุช:ูุดฺฉู)
(ุจุนุฏ:ุจุนุฏ)
(ููุงุฌู:ููุงุฌู)
(ุดููุฏ:ุดุฏ)
(.:.)
'''

url =  baseUrl + "Stemmer/LemmatizeWords2Phrase"
payload = u'["ุฏุฑุงููุฑุฏุงู", "ุฌุฒุงุฑ", "ูุฑุดุชฺฏุงู", "ุชููุง"]'
result = json.loads(callApi(url, payload, tokenKey))
for phrase in result:
    print("("+phrase['word']+":"+phrase['firstRoot']+") ")
''' result:
(ุฏุฑุงููุฑุฏุงู:ุฏุฑุงููุฑุฏ)
(ุฌุฒุงุฑ:ุฌุฒุฑู)
(ูุฑุดุชฺฏุงู:ูุฑุดุชู)
(ุชููุง:ุชููุง)
'''

#################### Call Spell Corrector ########################
url =  baseUrl + "TextRefinement/SpellCorrector"
payload = u'''{\"text\": \"ูููู ุจุง ูุจุงุช ูุฌุณุจุฏ\",
            \"checkSlang\": true, 
            \"normalize\": true, 
            \"candidateCount\": 2}'''
print(callApi(url, payload, tokenKey))
# result: ูููู ุจุง {ูุจุงุช,ููุงุช} {ูโฺุณุจุฏ,ูโุฌูุจุฏ}

################ Call Spell Corrector in Context ##################
url =  baseUrl + "TextRefinement/SpellCorrectorInContext"
payload = u'''{\"text\": \"ุณุชุฑ ุญูุงู ุงุณุช ฺฉู ุฏุฑ ุตุญุฑุง ุจุง ููุฏุงุฑ ฺฉู ุขุจ ุฒูุฏฺฏ ูฺฉูุฏ\",
            \"normalize\": true, 
            \"candidateCount\": 3}'''
print(callApi(url, payload, tokenKey))
# result: {ุดุชุฑ,ุณุทุฑ,ุณูุฑ} {ุญูุงู,ุญูุงู,ููุงู} {ุงุณุช,ุฏุณุช,ูุณุช} ฺฉู ุฏุฑ {ุตุญุฑุง,ุตูุฑุง,ุตุฏุฑุง} ุจุง {ููุฏุงุฑ,ูุฏุงุฑ,ููุฏุฑ} ฺฉู ุขุจ {ุฒูุฏฺฏ,ุจูุฏฺฏ,ุฒุฏฺฏ} {ูโฺฉูุฏ,ูโฺฉูุฏ,ูฺฉูุฏ}

################## Call Swear Word Detector ######################
url =  baseUrl + "TextRefinement/SwearWordTagger"
payload = u'"ุฎูููููููุฑุฑุฑุฑุฑูุง ุฏููููููููู  -   ุตฺฉุณ  ุณ.ฺฉ.ุณ   \r\n ุจูพุฏุฑููุงุฏุฑ"'
result = json.loads(callApi(url, payload, tokenKey))
## for item in result: ...
print(result)
# result: {'ุฎุฑุฑุฑุฑุฑูุง': 'MildSwearWord', 'ุฏููููููููู': 'MildSwearWord', 'ุตฺฉุณ': 'StrongSwearWord', 'ุณ.ฺฉ.ุณ': 'StrongSwearWord', ' ุจูพุฏุฑููุงุฏุฑ': 'StrongSwearWord'}

################ Call Slang to Formal Converter ##################
url =  baseUrl + "TextRefinement/FormalConverter"
payload = u'''"ุงฺฏู ุงูู ฺฏุฒูู ุฑู ฺฉูฺฉ ฺฉููุ ู ูพูุฌุฑู ุจุงุฒ ูุดู ฺฉู ูุชููู ุฑูุฒ ุนุจูุฑุชูู ุฑู ุงููุฌุง ุชุบุฑ ุจุฏู
    ุฏุงุดุชู ูู ุฑูุชู ุจุฑูุ ุฏูุฏู ฺฏุฑูุช ูุดุณุชุ ฺฏูุชู ุจุฐุงุฑ ุจูพุฑุณู ุจุจููู ููุงุฏ ูููุงุฏ ุฏูุฏู ููฺฏู ูููุฎูุงู ุจูุงู ุจุฐุงุฑ ุจุฑู ุจฺฏูุฑู ุจุฎูุงุจู ููุชููู ุจุดูู.
    ฺฉุชุงุจุง ุฎูุฏุชููู
    ููุฏููู ฺ ุจฺฏู ฺฉู ุฏฺฏู ุงููุฌุง ูุฑู
    ุณุงุนุช ฺู ูุชููู ุจุงูุ"'''
print(callApi(url, payload, tokenKey))
''' result:
ุงฺฏุฑ ุขู ฺฏุฒูู ุฑุง ฺฉูฺฉ ฺฉูุฏุ ฺฉ ูพูุฌุฑู ุจุงุฒ ูโุดูุฏ ฺฉู ูโุชูุงูุฏ ุฑูุฒ ุนุจูุฑุชุงู ุฑุง ุขูุฌุง ุชุบุฑ ุจุฏูุฏ
ุฏุงุดุชู ูโุฑูุชู ุจุฑููุ ุฏุฏู ฺฏุฑูุช ูุดุณุชุ ฺฏูุชู ุจฺฏุฐุงุฑ ุจูพุฑุณู ุจุจูู ูโุขุฏ ููโุขุฏ ุฏุฏู ูโฺฏูุฏ ููโุฎูุงูู ุจุงู ุจฺฏุฐุงุฑ ุจุฑูู ุจฺฏุฑู ุจุฎูุงุจู ููโุชูุงูู ุจูุดูู.
ฺฉุชุงุจโูุง ุฎูุฏุชุงู ุงุณุช
ููโุฏุงูู ฺู ุจฺฏูู ฺฉู ุฏฺฏุฑ ุขูุฌุง ูุฑูุฏ
ุณุงุนุช ฺูุฏ ูโุชูุงูุฏ ุจุงุฏุ
'''

######################## Call POS-Tagger ############################
url =  baseUrl + "PosTagger/GetPos"
payload = u'"ุงุญูุฏ ู ุนู ุจู ูุฏุฑุณู ูพุงู ุฎุงุจุงู ู ุฑูุชูุฏ"'
result = json.loads(callApi(url, payload, tokenKey))
for phrase in result:
    print("("+phrase['word']+","+phrase['tags']['POS']['item1']+") ")
''' result:
(ุงุญูุฏ,N)
(ู,CON)
(ุนู,N)
(ุจู,P)
(ูุฏุฑุณู,N)
(ูพุงู,ADJ)
(ุฎุงุจุงู,N)
(ูโุฑูุชูุฏ,V)
(.,)
'''

############################ Call NER ###############################
url =  baseUrl + "NamedEntityRecognition/Detect"
payload = u'"ุงุญูุฏ ุนุจุงุณ ุจู ุชุญุตูุงุช ุฎูุฏ ุฏุฑ ุฏุงูุดฺฏุงู ุขุฒุงุฏ ุงุณูุงู ุฏุฑ ูุดูุฏ ุงุฏุงูู ุฏุงุฏ"'
result = json.loads(callApi(url, payload, tokenKey))
for phrase in result:
    print("("+phrase['word']+","+phrase['tags']['NER']['item1']+") ")
''' result:
{ุงุญูุฏ,B-PER}
{ุนุจุงุณ,I-PER}
{ุจู,O}
{ุชุญุตูุงุช,O}
{ุฎูุฏ,O}
{ุฏุฑ,O}
{ุฏุงูุดฺฏุงู,B-ORG}
{ุขุฒุงุฏ,I-ORG}
{ุงุณูุงู,I-ORG}
{ุฏุฑ,O}
{ูุดูุฏ,I-LOC}
{ุงุฏุงูู,O}
{ุฏุงุฏ,O}
'''

##################### Call Language Detection #######################
url =  baseUrl + "LanguageDetection/Predict"
payload = u'"ุดุงู ุจุณู ุง ูุฎ. ุณู ุณุฒ ุจูุบุงุฒููุงู ฺฏุชูุฑ ุดุงู. ุจู ุจู ูู ูุดู ุฑุฏ. ุณุงุบ ุงูู ุณุฒ ูุฆุฌู ุณุฒ. ูุฆุฌู ุณูุ ุงูุดุงููุงุฑ ูุฆุฌู ุฏุฑุ ุณูุงู ูุงุฑ ูุงุฑ ุณุฒู ฺฉ ูุฑ ูุฆุฌู ุฏุฑ. ุงุฎฺ"'
print(callApi(url, payload, tokenKey))
# result: azb

################## Call Sentiment Classification ####################
url =  baseUrl + "SentimentAnalyzer/SentimentClassifier" # output:  0:Negative  1:Neutral  2:Positive
payload = u"\"ุงุตูุง ุฎูุจ ูุจูุฏ\""
print(callApi(url, payload, tokenKey)) 
# result: 0

###################### Call Text Similarity #########################
url =  baseUrl + "TextSimilarity/ExtractSynonyms"
payload = u"\"ุงุญุณุงู\""
print(callApi(url, payload, tokenKey))
# result: ["ุงุญุณุงู","ูฺฉ ฺฉุฑุฏู","ูฺฉูฺฉุงุฑ","ุจุฎุดุด","ูฺฉ","ุฎูุจ","ูฺฉู","ุงูุนุงู","ูฺฉู","ุงฺฉุฑุงู","ุตูุน","ูุถู","ูุทู","ููุช","ูุฒู","ูุนูุช","ูฺฉ_ฺฉุฑุฏู","ุงุญุณุงู (ูุงู)","ุงุญุณุงู_(ูุงู)"]

url =  baseUrl + "TextSimilarity/GetMostSimilarWord"
payload = u'''{
    "word": "ุฑูุญุงู", 
    "topN": "50"}'''
print(callApi(url, payload, tokenKey))
# result: ["ุฏููุช","ุขูุง","ุงุญูุฏโูฺุงุฏ","ุฑุฆุณโุฌูููุฑ","ุฑุณโุฌูููุฑ","ุงูุชุฎุงุจุงุช","ุจุฑุฌุงู","ุงุตูุงุญุงุช","ุฎุงุชู","ุทูุจุงู","ุญุณู","ุชุฑุงููพ","ูฺฺฉุฑุฑุฑุฑู","ุงุณุชุบูุงุฑ_ููฺฉูุฏ","ุฌูุงุจ","ูุงุดู","ุฌูุงูฺฏุฑ","ูุฑุฏู","ุฑุงุณุชโุฌูููุฑ","ุงูุชุตุงุฏ","ุงูููุงุจ","ุงุฎุชูุงู_ุณูพุงู_ุงุฑุชุด","ูุฌูุณ","ุณุฎูุงู","ุญูุงุช","ุชุฏุจุฑ","ุณุงุณ","ุณุฎูุฑุงู","ุงุตูุงุญ","ุงุตููฺฏุฑุงุงู","ุธุฑู","ุงูุง","ุงูุชุฎุงุจุงุช","ูุณุฆููู","ุขูุฑฺฉุง","ุดูุฑุง","ุฌูููุฑ","ุฑูุณูุฌุงู","ููุช","ุจุฑุฎุฑูุดุฏูโุงูุฏ","ุฏููุช_ุบุฑุจฺฏุฑุง_ู_ุณุงุฒุดฺฏุฑ","ุฑูุจุฑ","ุฌุงูุนู","ฺฏูุชู","ูุทุฑุญ","ุฏฺฉุชุฑ","ููุงุธุฑู_ุฑูุญุงู_ุจุง_ุฎูุฏุด","ุงุตูุงุญโุทูุจ","ูุฑุตุช_ุจุดุชุฑ","ุงุจูุง_ุนู_ุงูุณุท_ูุงุดู"]

url =  baseUrl + "TextSimilarity/GetSyntacticDistance"
payload = u'{"string1": "ุงุฑุงู ูุง", "string2": "ุงุฑุงูุงู", "distanceFunc": 2}'  # JaccardDistance
print(callApi(url, payload, tokenKey))
# result: 0.333333343

url =  baseUrl + "TextSimilarity/GetStatisticalDistance"
payload = u'''{
    "string1": "ุฑูุญุงู", 
    "string2": "ุฌุณูุงู"}'''
print(callApi(url, payload, tokenKey))
# result: 0.905992568

url =  baseUrl + "TextSimilarity/SentenceSimilarityBipartiteMatching"
payload = u'''{
    "string1": "ุญููู ูุบูููุง ุจู ุงุฑุงู", 
    "string2": "ุญููุงุช ูุบููุงู ุจู ุงุฑุงู", 
    "distanceFunc": 2}'''  # JaccardDistance
print(callApi(url, payload, tokenKey))
# result: 0.80357146263122559

url =  baseUrl + "TextSimilarity/SentenceSimilarityWithIntersectionMatching"
payload = u'''{
    "string1": "ุญููู ูุบูููุง ุจู ุงุฑุงู", 
    "string2": "ุญููุงุช ูุบููุงู ุจู ุงุฑุงู", 
    "distanceFunc": 2,
    "minDistThreshold": 0.3}'''  # ุญุฏุงูู ูุงุตูู ุฏู ฺฉููู ุจุฑุง ุงูุทุจุงู (ฺฉุณุงู ูุฑุถ ูููุฏู) ุขููุง
print(callApi(url, payload, tokenKey))
# result: 0.75

url =  baseUrl + "TextSimilarity/SentenceSimilarityWithNGramMatching"
payload = u'''{
    "string1": "ุญููู ูุบูููุง ุจู ุงุฑุงู", 
    "string2": "ุญููุงุช ูุบููุงู ุจู ุงุฑุงู", 
    "distanceFunc": 2,
    "minDistThreshold": 0.3}'''  # ุญุฏุงูู ูุงุตูู ุฏู ฺฉููู ุจุฑุง ุงูุทุจุงู (ฺฉุณุงู ูุฑุถ ูููุฏู) ุขููุง
print(callApi(url, payload, tokenKey))
# result: 0.714285708963871

url =  baseUrl + "TextSimilarity/SentenceSimilarityWithNearDuplicateDetector"
payload = u'''{
    "string1": "ุญููู ูุบูููุง ุจู ุงุฑุงู", 
    "string2": "ุญููุงุช ูุบููุงู ุจู ุงุฑุงู"}'''
print(callApi(url, payload, tokenKey))
# result: 0.5

################ Call Information Retrieval Function(s) ##################
url =  baseUrl + "InformationRetrieval/KeywordExtraction"
payload = u'''{
    "text": "ุณุฑูุงู ฺฏุฐุงุฑ ููฺฏูุช ุงูุงุฑุงุช ุฏุฑ ุชูุณุนู ุงูุฑฺ
ุงูุงุฑุงุช ุฏุฑ ููุงุทู ุดูุงู ุงู ฺฉุดูุฑ ุจุด ุงุฒ ทฐฐ ูููู ุฏุฑูู ุฏุฑ ุชูุณุนู ุงูุฑฺ ุณุฑูุงู ฺฏุฐุงุฑ ูโฺฉูุฏ.
ูุญูุฏ ุตุงูุญ ูุฏุฑฺฉู ุงุฏุงุฑู ุจุฑู ุงูุงุฑุงุช ุฏุฑ ูุดุณุช ุฎุจุฑ ุงุนูุงู ฺฉุฑุฏ ฺฉู ุงูุงุฑุงุช ูุชุญุฏู ุนุฑุจ ุงุฒ ุงูุณุงู ุชุง ุณุงู ฒฐฒฒ ููุงุฏ ธ ูพุฑูฺู ููู ุงูุฑฺ ุจู ููุธูุฑ ุชูุณุนู ู ฺฏุณุชุฑุด ุจุฑู ููุงุทู ุดูุงู ุงูุงุฑุงุช ุงุฌุฑุง ุฎูุงูุฏ ฺฉุฑุฏ.
ู ุงูุฒูุฏ: ุงู ุทุฑุญโูุง ุจุง ูุฒููโุง ุจุงูุบ ุจุฑ ทฐฐ ูููู ุฏุฑูู ุงุญุฏุงุซ ู ุชฺฉูู ุฎูุงูุฏ ุดุฏ.", 
    "minWordLength": 3,
    "maxWordCount": 3,
    "minKeywordFrequency": 1,
    "resultKeywordCount": 5,
    "method": "TFIDF"}'''
print(callApi(url, payload, tokenKey))
''' result:
{
    "ุงูุงุฑุงุช": 75.26119828977285,
    "ุชูุณุนู ุงูุฑฺ": 23.99621570204974,
    "ทฐฐ ูููู ุฏุฑูู": 21.95849183879027,
    "ููุงุทู ุดูุงู": 20.25414682800825,
    "ุณุฑูุงูโฺฏุฐุงุฑ ููฺฏูุช ุงูุงุฑุงุช": 17.670494399999686
}
'''

url =  baseUrl + "InformationRetrieval/StopWordRemoval"
payload = u'''"ุชู ูุชู ฺฉุงู ูุงุฑุณโุงุฑ ุจุง ูุฌููุนูโุง ุงุฒ ูุงุฑุบ ุงูุชุญุตูุงู ุฏุงูุดฺฏุงูโูุง ุตูุนุช ุดุฑูุ ุชุฑุจุช ูุฏุฑุณ ู ูุฑุฏูุณ ูุดูุฏ ุงุฒ ุณุงู ฑณนฐ ุจุตูุฑุช ุชุฎุตุต ุฏุฑ ุฒููู ูพุฑุฏุงุฒุด ุฒุจุงู ุทุจุน ูุดุบูู ุจู ูุนุงูุช ุงุณุช. ุฏุฑ ุณุงู ฑณนถ ุฏุฑ ุฌูุช ูุนุงูุช ูพฺููุด ุนููยญโุชุฑ ุฏุฑ ุฒููู ูพุฑุฏุงุฒุด ูุชูู ุจุฑุง ุฒุจุงู ูุงุฑุณุ ุงู ฺฏุฑูู ุจุง ุขุฒูุงุดฺฏุงู ูุชู ฺฉุงู ู ุงุฏฺฏุฑ ูุงุดู ูพฺููุดฺฏุงู ุนููู ู ููุงูุฑ ุงุทูุงุนุงุช ุงุฑุงู (ุงุฑุงูุฏุงฺฉ) ููฺฉุงุฑ ุชูฺฏุงุชูฺฏ ุฏุงุดุชู ุงุณุช."'''
print(callApi(url, payload, tokenKey))
# result: ุชู ูุชู ฺฉุงู ูุงุฑุณโุงุฑ ูุฌููุนูโุง ูุงุฑุบ ุงูุชุญุตูุงู ุฏุงูุดฺฏุงูโูุง ุตูุนุช ุดุฑูุ ุชุฑุจุช ูุฏุฑุณ ูุฑุฏูุณ ูุดูุฏ ุณุงู ฑณนฐ ุชุฎุตุต ูพุฑุฏุงุฒุด ุฒุจุงู ุทุจุน ูุดุบูู ูุนุงูุช. ุณุงู ฑณนถ ูุนุงูุช ูพฺููุด ุนููโุชุฑ ูพุฑุฏุงุฒุด ูุชูู ุฒุจุงู ูุงุฑุณุ ฺฏุฑูู ุขุฒูุงุดฺฏุงู ูุชู ฺฉุงู ุงุฏฺฏุฑ ูุงุดู ูพฺููุดฺฏุงู ุนููู ููุงูุฑ ุงุทูุงุนุงุช ุงุฑุงู (ุงุฑุงูุฏุงฺฉ) ููฺฉุงุฑ ุชูฺฏุงุชูฺฏ.

######################## Call Virastar ############################
url =  baseUrl + "Virastar/ScanText"
payload = u'{"text": "ุญุชูุง ุขู ูุง ูููู ุฑุง ุงุญุชุฑุงู ู ฺชููุฏ. ฺฏ ุจูพูุฌุฑู  ุจุฒฺฏ ูุณุจุฒ ุจุงุฒ ูุดูุฏ . !ุญุถูุฑ ุชุงู ุฑุง ฺฉุฑุงู ู ุฏุงุดุชู", "returnOnlyChangedTokens": false}'
result = json.loads(callApi(url, payload, tokenKey))
output = ''
for token in result:
    output += token['originalText']
    if (token['isChanged']):
        output += "{" + token['editList'][0]['suggestedText'] + "(" + token['editList'][0]['description'] + ")"
        if(len(token['editList']) > 1):
            iterEdit = iter(token['editList'])
            next(iterEdit)
            for edit in iterEdit:
                output += " - " + edit['suggestedText'] + "(" + edit['description'] + ")"
        output += "}"
print(output)
''' result:
ุญุชูุง{ุญุชูุงู(ุงุตูุงุญ ุชููู)} ุขู ูุง{ุขูโูุง(ุงุตูุงุญ ูพุณููุฏ)} ูููู{ูุคูู(ุงุตูุงุญ ุญุฑูู ุฏุงุฑุง ููุฒู)} ุฑุง ุงุญุชุฑุงู ู ฺชููุฏ{ูโฺฉููุฏ(ุงุตูุงุญ ูพุดููุฏุ ุงุตูุงุญุงุช ููุณูโูุง)}. ฺฏ{ฺฉ(ุงุตูุงุญ ุงุดุชุจุงู ุชุงูพ/ุงููุงุฆ ุฑุงุฌ)} ุจูพูุฌุฑู {ุจูพูุฌุฑูโ(ุงุตูุงุญ ูพุณููุฏ) - ุจู ูพูุฌุฑูโ(ุฌุฏุงุณุงุฒ ูุงฺูโูุง ุจูู ฺุณุจุฏู)} ุจุฒฺฏ{ุจุฒุฑฺฏ(ุงุตูุงุญ ุงุดุชุจุงู ุชุงูพ/ุงููุงุฆ ุฑุงุฌ)} ูุณุจุฒ{ู ุณุจุฒ(ุฌุฏุงุณุงุฒ ูุงฺูโูุง ุจูู ฺุณุจุฏู)} ุจุงุฒ ูุดูุฏ{ูโุดูุฏ(ุงุตูุงุญ ูพุดููุฏ)} . !{.! (ุงุตูุงุญ ูุงุตููโฺฏุฐุงุฑ ูุทุงุจู ุฏุณุชูุฑ ุฒุจุงู)}ุญุถูุฑ ุชุงู{ุญุถูุฑุชุงู(ูพุดููุงุฏ ูพูุณุชูโููุณ ูุงฺูโูุง ฺูุฏุจุฎุด)} ุฑุง ฺฉุฑุงู{ฺฉุฑู(ุงู ูุงฺู ุงุฒ ูุธุฑ ูุบู ูุดุงุจู ยซฺฉุฑูยป ุงุณุช. (ุจุง ูุฒุงู ูุงุตู: 1)) - ุฑุงู(ุงู ูุงฺู ุงุฒ ูุธุฑ ูุบู ูุดุงุจู ยซุฑุงูยป ุงุณุช. (ุจุง ูุฒุงู ูุงุตู: 0.98)) - ฺฏุฑุงู(ุงู ูุงฺู ุงุฒ ูุธุฑ ูุบู ูุดุงุจู ยซฺฏุฑุงูยป ุงุณุช. (ุจุง ูุฒุงู ูุงุตู: 0.75))} ู ุฏุงุดุชู{ูโุฏุงุดุชู(ุงุตูุงุญ ูพุดููุฏ)}
'''
