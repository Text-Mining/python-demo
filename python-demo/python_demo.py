# -*- coding: utf-8 -*-
try:
    # Fix UTF8 output issues on Windows console.
    # Does nothing if package is not installed
    from win_unicode_console import enable
    enable()
except ImportError:
    pass

def utfReverse(s):
    # CAVEAT: this will mess up characters that are
    # more than 2 bytes long in utf 16
    u = s.decode("utf-8")
    return u[::-1].encode("utf-8")

######################################################################

import requests
import json

def callApi(url, data, tokenKey):    
    headers = {
        'Content-Type': "application/json",
        'Authorization': "Bearer " + tokenKey,
        'Cache-Control': "no-cache"
    }
    response = requests.request("POST", url, data=data.encode("utf-8"), headers=headers)
    return response.text    # return utfReverse(response.text.encode("utf-8"))
    
##################### Get Token by Api Key ##########################
baseUrl = "http://api.text-mining.ir/api/"
url = baseUrl + "Token/GetToken"
querystring = {"apikey":"YOUR_API_KEY"}
response = requests.request("GET", url, params=querystring)
data = json.loads(response.text)
tokenKey = data['token']

######################## Call Normalizer ############################
url =  baseUrl + "PreProcessing/NormalizePersianWord"
payload = u"{\"text\":\"ÙˆÙ„Ù€Ù€Û’ Ø§Ú¯Ù€Ù€Ø± Ø¯ÚªÙ€Ù€Ù…Ù€Ù€Ù‡ Ù…Ù€Ù€ÚªÙ€Ù€Ø« Ø±Ùˆ Ù„Ù€Ù€Ù…Ù€Ù€Ø³ ÚªÙ€Ù€Ù†Ù€Ù€ÛŒÙ€Ù€Ù… ÚªÙ€Ù€Ù„Ù€Ù€Ø§ Ù…Ù€Ù€ØªÙ€Ù€Ù† Ú†Ù€Ù€Ù†Ù€Ù€Ø¯ÛŒÙ€Ù€Ù† ØµÙ€Ù€ÙÙ€Ù€Ø­Ù€Ù€Ù‡ Ø¬Ù€Ù€Ø§Ø¨Ù€Ù€Ù‡ Ø¬Ù€Ù€Ø§ Ù…Ù€Ù€ÛŒÙ€Ù€Ø´Ù€Ù€Ù‡ Ùˆ Ø¯ÛŒÙ€Ù€Ú¯Ù€Ù€Ù‡ Ù†Ù€Ù€Ù…Ù€Ù€ÛŒÙ€Ù€Ø´Ù€Ù€Ù‡ ÙÙ€Ù€Ù‡Ù…Ù€Ù€ÛŒÙ€Ù€Ø¯ ÚªÙ€Ù€Ø¯ÙˆÙ… Ø¢ÛŒÙ€Ù€Ù‡ ØªÙ€Ù€Ù„Ù€Ù€Ø§ÙˆØª Ù…Ù€Ù€ÛŒ Ø´ÙˆØ¯ Ø¨Ù€Ù€Ø§ÛŒÙ€Ù€Ø¯ Ú†Ù€Ù€Û’ ÚªÙ€Ù€Ù†Ù€Ù€ÛŒÙ€Ù€Ù…ØŸ.\", \"refineSeparatedAffix\":true}"
print(callApi(url, payload, tokenKey))
# result: ÙˆÙ„ÛŒ Ø§Ú¯Ø± Ø¯Ú©Ù…Ù‡ Ù…Ú©Ø« Ø±Ùˆ Ù„Ù…Ø³ Ú©Ù†ÛŒÙ… Ú©Ù„Ø§ Ù…ØªÙ† Ú†Ù†Ø¯ÛŒÙ† ØµÙØ­Ù‡ Ø¬Ø§Ø¨Ù‡ Ø¬Ø§ Ù…ÛŒØ´Ù‡ Ùˆ Ø¯ÛŒÚ¯Ù‡ Ù†Ù…ÛŒØ´Ù‡ ÙÙ‡Ù…ÛŒØ¯ Ú©Ø¯ÙˆÙ… Ø¢ÛŒÙ‡ ØªÙ„Ø§ÙˆØª Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¨Ø§ÛŒØ¯ Ú†ÛŒ Ú©Ù†ÛŒÙ…ØŸ.

##################### Call Sentence Splitter ########################
url =  baseUrl + "PreProcessing/SentenceSplitter"
payload = u'''{\"text\": \"Ù…Ù† Ø¨Ø§ Ø¯ÙˆØ³ØªÙ… Ø¨Ù‡ Ù…Ø¯Ø±Ø³Ù‡ Ù…ÛŒ Ø±ÙØªÛŒÙ… Ùˆ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ ØªØ­ØµÛŒÙ„ Ø¨ÙˆØ¯ÛŒÙ…. Ø³Ù¾Ø³ Ø¨Ù‡ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø±Ø§Ù‡ ÛŒØ§ÙØªÛŒÙ…\",
    \"checkSlang\": true, 
    \"normalize\": true, 
    \"normalizerParams\": {
        \"text\": \"don't care\",
        \"RefineQuotationPunc \": false
    },
    \"complexSentence\": true
}'''
print(callApi(url, payload, tokenKey))
# resuilt: ["Ù…Ù† Ø¨Ø§ Ø¯ÙˆØ³ØªÙ… Ø¨Ù‡ Ù…Ø¯Ø±Ø³Ù‡ Ù…ÛŒâ€ŒØ±ÙØªÛŒÙ…","Ùˆ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ ØªØ­ØµÛŒÙ„ Ø¨ÙˆØ¯ÛŒÙ… .","Ø³Ù¾Ø³ Ø¨Ù‡ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø±Ø§Ù‡ ÛŒØ§ÙØªÛŒÙ…"

######################## Call Tokenizer ############################
url =  baseUrl + "PreProcessing/Tokenize"
payload = u"\"Ù…Ù† Ø¨Ø§ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯ Ú©Ø±Ø¯Ù…\""
print(callApi(url, payload, tokenKey))
# rsult: ["Ù…Ù†","Ø¨Ø§","Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù†","Ø¯ÛŒÚ¯Ø±ÛŒ","Ø¨Ø±Ø®ÙˆØ±Ø¯","Ú©Ø±Ø¯Ù…"]

url =  baseUrl + "PreProcessing/TokenizeWithType"
payload = u"\"Ø§Ø®Ø¨Ø§Ø± 20:30 Ù…ÙˆØ±Ø® 1398/2/22 Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ Ø´Ø±Ú©Øª T.E.T Ù…Ø¨Ù„Øº 200.57 Ù…ÛŒÙ„ÛŒÙˆÙ† Ø§Ø±Ø²Ø´ Ø¯Ø§Ø±Ø¯!!!  ğŸ˜’ @Khabar_Alaki -- email: hi@text-mining.ir\""
print(callApi(url, payload, tokenKey))
#result: [{"key":"Ø§Ø®Ø¨Ø§Ø±","value":"Word"},{"key":"20:30 ","value":"DateTime"},{"key":"Ù…ÙˆØ±Ø®","value":"Word"},{"key":"1398/2/22","value":"DateTime"},{"key":"Ø§Ø¹Ù„Ø§Ù…","value":"Word"},{"key":"Ú©Ø±Ø¯","value":"Word"},{"key":"Ø´Ø±Ú©Øª","value":"Word"},{"key":"T.E.T","value":"Abbreviation"},{"key":"Ù…Ø¨Ù„Øº","value":"Word"},{"key":"200.57","value":"Number"},{"key":"Ù…ÛŒÙ„ÛŒÙˆÙ†","value":"Word"},{"key":"Ø§Ø±Ø²Ø´","value":"Word"},{"key":"Ø¯Ø§Ø±Ø¯","value":"Word"},{"key":"!!!","value":"Separator"},{"key":"ğŸ˜’","value":"Emoji"},{"key":"@Khabar_Alaki","value":"SocialId"},{"key":"--","value":"Separator"},{"key":"email","value":"Word"},{"key":":","value":"Separator"},{"key":"hi@text-mining.ir","value":"Email"}]

############# Call Sentence Splitter and Tokenizer #################
url =  baseUrl + "PreProcessing/SentenceSplitterAndTokenize"
payload = u'''{\"text\": \"Ù…Ù† Ø¨Ø§ Ø¯ÙˆØ³ØªÙ… Ø¨Ù‡ Ù…Ø¯Ø±Ø³Ù‡ Ù…ÛŒ Ø±ÙØªÛŒÙ… Ùˆ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ ØªØ­ØµÛŒÙ„ Ø¨ÙˆØ¯ÛŒÙ…. Ø³Ù¾Ø³ Ø¨Ù‡ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø±Ø§Ù‡ ÛŒØ§ÙØªÛŒÙ…\",
    \"checkSlang\": true, 
    \"normalize\": true, 
    \"normalizerParams\": {
        \"text\": \"don't care\",
        \"replaceWildChar\": true,
        \"replaceDigit\": true,
        \"refineSeparatedAffix\": true,
        \"refineQuotationPunc\": false
    },
    \"complexSentence\": true
}'''
print(callApi(url, payload, tokenKey))
# result: [["Ù…Ù†","Ø¨Ø§","Ø¯ÙˆØ³ØªÙ…","Ø¨Ù‡","Ù…Ø¯Ø±Ø³Ù‡","Ù…ÛŒâ€ŒØ±ÙØªÛŒÙ…"],["Ùˆ","Ø¯Ø±","Ø¢Ù†Ø¬Ø§","Ù…Ø´ØºÙˆÙ„","Ø¨Ù‡","ØªØ­ØµÛŒÙ„","Ø¨ÙˆØ¯ÛŒÙ…","."],["Ø³Ù¾Ø³","Ø¨Ù‡","Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡","Ø±Ø§Ù‡","ÛŒØ§ÙØªÛŒÙ…"]]

########################## Call Stemmer ##########################
url =  baseUrl + "Stemmer/LemmatizeText2Text"
payload = u'"Ù…Ù† Ø¨Ø§ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯ Ú©Ø±Ø¯Ù…. Ø³Ù¾Ø³ Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ Ú¯ÙØªÙ…\nÙ…Ù† Ø¨Ø§ Ø´Ù…Ø§ Ú©Ø§Ø±Ù‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø±Ù…"'
print(callApi(url, payload, tokenKey))
''' result: 
Ù…Ù† Ø¨Ø§ Ø¯Ø§Ù†Ø´Ø¬Ùˆ Ø¯ÛŒÚ¯Ø± Ø¨Ø± Ø®ÙˆØ±Ø¯ Ú©Ø±Ø¯. Ø³Ù¾Ø³ Ø¨Ù‡ Ø¢Ù† Ú¯ÙØª
Ù…Ù† Ø¨Ø§ Ø´Ù…Ø§ Ú©Ø§Ø± Ø²ÛŒØ§Ø¯ Ø¯Ø§Ø´Øª
'''

url =  baseUrl + "Stemmer/LemmatizePhrase2Phrase"
payload = u'{"phrases": [{ "word": "Ø¯Ø±ÛŒØ§Ù†ÙˆØ±Ø¯Ø§Ù†ÛŒ" }, { "word": "ÙØ±Ø´ØªÚ¯Ø§Ù†" }], "checkSlang": false}'
print(callApi(url, payload, tokenKey))
# result: [{"wordComment":"","simplePos":"","rootWords":["Ø¯Ø±ÛŒØ§Ù†ÙˆØ±Ø¯","Ø¯Ø±ÛŒØ§"],"verbInformation":null,"sentenceNumber":0,"wordNumberInSentence":0,"startCharIndex":0,"word":"Ø¯Ø±ÛŒØ§Ù†ÙˆØ±Ø¯Ø§Ù†ÛŒ","tags":{},"firstRoot":"Ø¯Ø±ÛŒØ§Ù†ÙˆØ±Ø¯","wordCount":1,"length":11,"isVerb":false,"isPunc":false},{"wordComment":"","simplePos":"","rootWords":["ÙØ±Ø´ØªÙ‡"],"verbInformation":null,"sentenceNumber":0,"wordNumberInSentence":0,"startCharIndex":0,"word":"ÙØ±Ø´ØªÚ¯Ø§Ù†","tags":{},"firstRoot":"ÙØ±Ø´ØªÙ‡","wordCount":1,"length":7,"isVerb":false,"isPunc":false}]

url =  baseUrl + "Stemmer/LemmatizeText2Phrase"
payload = u'{"text": "Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ù‡ Ù…Ø¯Ø§Ø±Ø³ Ø§Ø³ØªØ¹Ø¯Ø§Ø¯Ù‡Ø§ÛŒ Ø¯Ø±Ø®Ø´Ø§Ù† Ø±Ø§Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯ Ú©Ø±Ø¯ Ú©Ù‡ Ø¨Ø§ Ù…Ø´Ú©Ù„Ø§Øª Ø¨Ø¹Ø¯ÛŒ Ù…ÙˆØ§Ø¬Ù‡ Ø´ÙˆÙ†Ø¯.", "checkSlang": false}'
result = json.loads(callApi(url, payload, tokenKey))
for phrase in result:
    print("("+phrase['word']+":"+phrase['firstRoot']+") ")
''' result:
(Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù†:Ø¯Ø§Ù†Ø´Ø¬Ùˆ)
(Ø²ÛŒØ§Ø¯ÛŒ:Ø²ÛŒØ§Ø¯)
(Ø¨Ù‡:Ø¨Ù‡)
(Ù…Ø¯Ø§Ø±Ø³:Ù…Ø¯Ø±Ø³Ù‡)
(Ø§Ø³ØªØ¹Ø¯Ø§Ø¯Ù‡Ø§ÛŒ:Ø§Ø³ØªØ¹Ø¯Ø§Ø¯)
(Ø¯Ø±Ø®Ø´Ø§Ù†:Ø¯Ø±Ø®Ø´Ø§Ù†)
(Ø±Ø§Ù‡:Ø±Ø§Ù‡)
(Ù¾ÛŒØ¯Ø§:Ù¾ÛŒØ¯Ø§)
(Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯ Ú©Ø±Ø¯:Ù†Ú©Ø±Ø¯)
(Ú©Ù‡:Ú©Ù‡)
(Ø¨Ø§:Ø¨Ø§)
(Ù…Ø´Ú©Ù„Ø§Øª:Ù…Ø´Ú©Ù„)
(Ø¨Ø¹Ø¯ÛŒ:Ø¨Ø¹Ø¯)
(Ù…ÙˆØ§Ø¬Ù‡:Ù…ÙˆØ§Ø¬Ù‡)
(Ø´ÙˆÙ†Ø¯:Ø´Ø¯)
(.:.)
'''

url =  baseUrl + "Stemmer/LemmatizeWords2Phrase"
payload = u'["Ø¯Ø±ÛŒØ§Ù†ÙˆØ±Ø¯Ø§Ù†ÛŒ", "Ø¬Ø²Ø§ÛŒØ±", "ÙØ±Ø´ØªÚ¯Ø§Ù†", "ØªÙ†Ù‡Ø§"]'
result = json.loads(callApi(url, payload, tokenKey))
for phrase in result:
    print("("+phrase['word']+":"+phrase['firstRoot']+") ")
''' result:
(Ø¯Ø±ÛŒØ§Ù†ÙˆØ±Ø¯Ø§Ù†ÛŒ:Ø¯Ø±ÛŒØ§Ù†ÙˆØ±Ø¯)
(Ø¬Ø²Ø§ÛŒØ±:Ø¬Ø²ÛŒØ±Ù‡)
(ÙØ±Ø´ØªÚ¯Ø§Ù†:ÙØ±Ø´ØªÙ‡)
(ØªÙ†Ù‡Ø§:ØªÙ†Ù‡Ø§)
'''

#################### Call Spell Corrector ########################
url =  baseUrl + "TextRefinement/SpellCorrector"
payload = u'''{\"text\": \"ÙÙ‡ÙˆÙ‡ Ø¨Ø§ Ù…Ø¨Ø§Øª Ù…ÛŒØ¬Ø³Ø¨Ø¯\",
            \"checkSlang\": true, 
            \"normalize\": true, 
            \"candidateCount\": 2}'''
print(callApi(url, payload, tokenKey))
# result: Ù‚Ù‡ÙˆÙ‡ Ø¨Ø§ {Ù†Ø¨Ø§Øª,Ù…Ù„Ø§Øª} {Ù…ÛŒâ€ŒÚ†Ø³Ø¨Ø¯,Ù…ÛŒâ€ŒØ¬Ù†Ø¨Ø¯}

################ Call Spell Corrector in Context ##################
url =  baseUrl + "TextRefinement/SpellCorrectorInContext"
payload = u'''{\"text\": \"Ø³ØªØ± Ø­ÛŒÙˆØ§Ù†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± ØµØ­Ø±Ø§ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± Ú©Ù… Ø¢Ø¨ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒÚ©Ù†Ø¯\",
            \"normalize\": true, 
            \"candidateCount\": 3}'''
print(callApi(url, payload, tokenKey))
# result: {Ø´ØªØ±,Ø³Ø·Ø±,Ø³ÙØ±} {Ø­ÛŒÙˆØ§Ù†ÛŒ,Ø­ÛŒÙˆØ§Ù†,ÛŒÙˆÙ†Ø§Ù†ÛŒ} {Ø§Ø³Øª,Ø¯Ø³Øª,Ù‡Ø³Øª} Ú©Ù‡ Ø¯Ø± {ØµØ­Ø±Ø§,ØµÙØ±Ø§,ØµØ¯Ø±Ø§} Ø¨Ø§ {Ù…Ù‚Ø¯Ø§Ø±,Ù…Ø¯Ø§Ø±,Ù…Ù‚Ø¯Ø±} Ú©Ù… Ø¢Ø¨ {Ø²Ù†Ø¯Ú¯ÛŒ,Ø¨Ù†Ø¯Ú¯ÛŒ,Ø²Ø¯Ú¯ÛŒ} {Ù…ÛŒâ€ŒÚ©Ù†Ø¯,Ù…ÛŒâ€ŒÚ©Ù†Ø¯,Ù…Ú©Ù†Ø¯}

################## Call Swear Word Detector ######################
url =  baseUrl + "TextRefinement/SwearWordTagger"
payload = u'"Ø®Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø±Ø±Ø±Ø±Ø±Ù‡Ø§ÛŒ Ø¯ÛŒÙˆÙˆÙˆÙˆÙˆÙ†Ù‡Ù‡Ù‡  -   ØµÚ©Ø³  Ø³.Ú©.Ø³ ÛŒ  \r\n Ø¨ÛŒÙ¾Ø¯Ø±ÙˆÙ…Ø§Ø¯Ø±"'
result = json.loads(callApi(url, payload, tokenKey))
## for item in result: ...
print(result)
# result: {'Ø®Ø±Ø±Ø±Ø±Ø±Ù‡Ø§ÛŒ': 'MildSwearWord', 'Ø¯ÛŒÙˆÙˆÙˆÙˆÙˆÙ†Ù‡Ù‡Ù‡': 'MildSwearWord', 'ØµÚ©Ø³': 'StrongSwearWord', 'Ø³.Ú©.Ø³': 'StrongSwearWord', 'ÛŒ Ø¨ÛŒÙ¾Ø¯Ø±ÙˆÙ…Ø§Ø¯Ø±': 'StrongSwearWord'}

################ Call Slang to Formal Converter ##################
url =  baseUrl + "TextRefinement/FormalConverter"
payload = u'''"Ø§Ú¯Ù‡ Ø§ÙˆÙ† Ú¯Ø²ÛŒÙ†Ù‡ Ø±Ùˆ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒÙ†ØŒ ÛŒÙ‡ Ù¾Ù†Ø¬Ø±Ù‡ Ø¨Ø§Ø² Ù…ÛŒØ´Ù‡ Ú©Ù‡ Ù…ÛŒØªÙˆÙ†ÛŒÙ† Ø±Ù…Ø² Ø¹Ø¨ÙˆØ±ØªÙˆÙ† Ø±Ùˆ Ø§ÙˆÙ†Ø¬Ø§ ØªØºÛŒÛŒØ± Ø¨Ø¯ÛŒÙ†
    Ø¯Ø§Ø´ØªÙ… Ù…ÙŠ Ø±ÙØªÙ… Ø¨Ø±Ù…ØŒ Ø¯ÙŠØ¯Ù… Ú¯Ø±ÙØª Ù†Ø´Ø³ØªØŒ Ú¯ÙØªÙ… Ø¨Ø°Ø§Ø± Ø¨Ù¾Ø±Ø³Ù… Ø¨Ø¨ÙŠÙ†Ù… Ù…ÙŠØ§Ø¯ Ù†Ù…ÙŠØ§Ø¯ Ø¯ÙŠØ¯Ù… Ù…ÙŠÚ¯Ù‡ Ù†Ù…ÙŠØ®ÙˆØ§Ù… Ø¨ÙŠØ§Ù… Ø¨Ø°Ø§Ø± Ø¨Ø±Ù… Ø¨Ú¯ÙŠØ±Ù… Ø¨Ø®ÙˆØ§Ø¨Ù… Ù†Ù…ÛŒØªÙˆÙ†Ù… Ø¨Ø´ÛŒÙ†Ù….
    Ú©ØªØ§Ø¨Ø§ÛŒ Ø®ÙˆØ¯ØªÙˆÙ†Ù‡
    Ù†Ù…ÛŒØ¯ÙˆÙ†Ù… Ú†ÛŒ Ø¨Ú¯Ù… Ú©Ù‡ Ø¯ÛŒÚ¯Ù‡ Ø§ÙˆÙ†Ø¬Ø§ Ù†Ø±Ù‡
    Ø³Ø§Ø¹Øª Ú†Ù† Ù…ÛŒØªÙˆÙ†ÛŒÙ† Ø¨ÛŒØ§ÛŒÛŒÙ†ØŸ"'''
print(callApi(url, payload, tokenKey))
''' result:
Ø§Ú¯Ø± Ø¢Ù† Ú¯Ø²ÛŒÙ†Ù‡ Ø±Ø§ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯ØŒ ÛŒÚ© Ù¾Ù†Ø¬Ø±Ù‡ Ø¨Ø§Ø² Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø±Ù…Ø² Ø¹Ø¨ÙˆØ±ØªØ§Ù† Ø±Ø§ Ø¢Ù†Ø¬Ø§ ØªØºÛŒÛŒØ± Ø¨Ø¯Ù‡ÛŒØ¯
Ø¯Ø§Ø´ØªÙ… Ù…ÛŒâ€ŒØ±ÙØªÙ… Ø¨Ø±ÙˆÙ…ØŒ Ø¯ÛŒØ¯Ù… Ú¯Ø±ÙØª Ù†Ø´Ø³ØªØŒ Ú¯ÙØªÙ… Ø¨Ú¯Ø°Ø§Ø± Ø¨Ù¾Ø±Ø³Ù… Ø¨Ø¨ÛŒÙ†Ù… Ù…ÛŒâ€ŒØ¢ÛŒØ¯ Ù†Ù…ÛŒâ€ŒØ¢ÛŒØ¯ Ø¯ÛŒØ¯Ù… Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù… Ø¨ÛŒØ§ÛŒÙ… Ø¨Ú¯Ø°Ø§Ø± Ø¨Ø±ÙˆÙ… Ø¨Ú¯ÛŒØ±Ù… Ø¨Ø®ÙˆØ§Ø¨Ù… Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ù†Ø´ÛŒÙ†Ù….
Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø³Øª
Ù†Ù…ÛŒâ€ŒØ¯Ø§Ù†Ù… Ú†Ù‡ Ø¨Ú¯ÙˆÛŒÙ… Ú©Ù‡ Ø¯ÛŒÚ¯Ø± Ø¢Ù†Ø¬Ø§ Ù†Ø±ÙˆØ¯
Ø³Ø§Ø¹Øª Ú†Ù†Ø¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨ÛŒØ§ÛŒÛŒØ¯ØŸ
'''

######################## Call POS-Tagger ############################
url =  baseUrl + "PosTagger/GetPos"
payload = u'"Ø§Ø­Ù…Ø¯ Ùˆ Ø¹Ù„ÛŒ Ø¨Ù‡ Ù…Ø¯Ø±Ø³Ù‡ Ù¾Ø§ÛŒÛŒÙ† Ø®ÛŒØ§Ø¨Ø§Ù† Ù…ÛŒ Ø±ÙØªÙ†Ø¯"'
result = json.loads(callApi(url, payload, tokenKey))
for phrase in result:
    print("("+phrase['word']+","+phrase['tags']['POS']['item1']+") ")
''' result:
(Ø§Ø­Ù…Ø¯,N)
(Ùˆ,CON)
(Ø¹Ù„ÛŒ,N)
(Ø¨Ù‡,P)
(Ù…Ø¯Ø±Ø³Ù‡,N)
(Ù¾Ø§ÛŒÛŒÙ†,ADJ)
(Ø®ÛŒØ§Ø¨Ø§Ù†,N)
(Ù…ÛŒâ€ŒØ±ÙØªÙ†Ø¯,V)
(.,)
'''

############################ Call NER ###############################
url =  baseUrl + "NamedEntityRecognition/Detect"
payload = u'"Ø§Ø­Ù…Ø¯ Ø¹Ø¨Ø§Ø³ÛŒ Ø¨Ù‡ ØªØ­ØµÛŒÙ„Ø§Øª Ø®ÙˆØ¯ Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¢Ø²Ø§Ø¯ Ø§Ø³Ù„Ø§Ù…ÛŒ Ø¯Ø± Ù…Ø´Ù‡Ø¯ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø¯"'
result = json.loads(callApi(url, payload, tokenKey))
for phrase in result:
    print("("+phrase['word']+","+phrase['tags']['NER']['item1']+") ")
''' result:
{Ø§Ø­Ù…Ø¯,B-PER}
{Ø¹Ø¨Ø§Ø³ÛŒ,I-PER}
{Ø¨Ù‡,O}
{ØªØ­ØµÛŒÙ„Ø§Øª,O}
{Ø®ÙˆØ¯,O}
{Ø¯Ø±,O}
{Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡,B-ORG}
{Ø¢Ø²Ø§Ø¯,I-ORG}
{Ø§Ø³Ù„Ø§Ù…ÛŒ,I-ORG}
{Ø¯Ø±,O}
{Ù…Ø´Ù‡Ø¯,I-LOC}
{Ø§Ø¯Ø§Ù…Ù‡,O}
{Ø¯Ø§Ø¯,O}
'''

##################### Call Language Detection #######################
url =  baseUrl + "LanguageDetection/Predict"
payload = u'"Ø´Ø§Ù… ÛŒÛŒØ¨Ø³Ù† ÛŒØ§ ÛŒÙˆØ®. Ø³Ù† Ø³ÛŒØ² Ø¨ÙˆØºØ§Ø²ÛŒÙ…Ù†Ø§Ù† Ú¯ØªÙ…ÛŒØ± Ø´Ø§Ù…. Ø¨Ù‡ Ø¨Ù‡ Ù†Ù‡ Ù‚Ø´Ù‡ ÛŒØ±Ø¯ÛŒ. Ø³Ø§Øº Ø§ÙˆÙ„ Ø³ÛŒØ² Ù†Ø¦Ø¬Ù‡ Ø³ÛŒØ². Ù†Ø¦Ø¬Ù‡ Ø³Ù†ØŸ Ø§ÙˆØ´Ø§Ù‚Ù„Ø§Ø± Ù†Ø¦Ø¬Ù‡ Ø¯ÛŒØ±ØŸ Ø³Ù„Ø§Ù… Ù„Ø§Ø±ÛŒ ÙˆØ§Ø± Ø³ÛŒØ²ÛŒÙ† Ú©ÛŒ Ù„Ø± Ù†Ø¦Ø¬Ù‡ Ø¯ÛŒØ±. ÛŒØ§Ø®Ú†ÛŒ"'
print(callApi(url, payload, tokenKey))
# result: azb

################## Call Sentiment Classification ####################
url =  baseUrl + "SentimentAnalyzer/SentimentClassifier" # output:  0:Negative  1:Neutral  2:Positive
payload = u"\"Ø§ØµÙ„Ø§ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯\""
print(callApi(url, payload, tokenKey)) 
# result: 0

###################### Call Text Similarity #########################
url =  baseUrl + "TextSimilarity/ExtractSynonyms"
payload = u"\"Ø§Ø­Ø³Ø§Ù†\""
print(callApi(url, payload, tokenKey))
# result: ["Ø§Ø­Ø³Ø§Ù†","Ù†ÛŒÚ©ÛŒ Ú©Ø±Ø¯Ù†","Ù†ÛŒÚ©ÙˆÚ©Ø§Ø±ÛŒ","Ø¨Ø®Ø´Ø´","Ù†ÛŒÚ©ÛŒ","Ø®ÙˆØ¨ÛŒ","Ù†ÛŒÚ©ÙˆÛŒÛŒ","Ø§Ù†Ø¹Ø§Ù…","Ù†Ú©ÙˆÛŒÛŒ","Ø§Ú©Ø±Ø§Ù…","ØµÙ†Ø¹","ÙØ¶Ù„","Ù„Ø·Ù","Ù…Ù†Øª","Ù†Ø²Ù„","Ù†Ø¹Ù…Øª","Ù†ÛŒÚ©ÛŒ_Ú©Ø±Ø¯Ù†","Ø§Ø­Ø³Ø§Ù† (Ù†Ø§Ù…)","Ø§Ø­Ø³Ø§Ù†_(Ù†Ø§Ù…)"]

url =  baseUrl + "TextSimilarity/GetMostSimilarWord"
payload = u'''{
    "word": "Ø±ÙˆØ­Ø§Ù†ÛŒ", 
    "topN": "50"}'''
print(callApi(url, payload, tokenKey))
# result: ["Ø¯ÙˆÙ„Øª","Ø¢Ù‚Ø§ÛŒ","Ø§Ø­Ù…Ø¯ÛŒâ€ŒÙ†Ú˜Ø§Ø¯","Ø±Ø¦ÛŒØ³â€ŒØ¬Ù…Ù‡ÙˆØ±","Ø±ÛŒÛŒØ³â€ŒØ¬Ù…Ù‡ÙˆØ±","Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª","Ø¨Ø±Ø¬Ø§Ù…","Ø§ØµÙ„Ø§Ø­Ø§Øª","Ø®Ø§ØªÙ…ÛŒ","Ø·Ù„Ø¨Ø§Ù†","Ø­Ø³Ù†","ØªØ±Ø§Ù…Ù¾","Ù…Ú†Ú©Ø±Ø±Ø±Ø±ÛŒÙ…","Ø§Ø³ØªØºÙØ§Ø±_Ù†Ù…ÛŒÚ©Ù†ÛŒØ¯","Ø¬Ù†Ø§Ø¨","Ù‡Ø§Ø´Ù…ÛŒ","Ø¬Ù‡Ø§Ù†Ú¯ÛŒØ±ÛŒ","Ù…Ø±Ø¯Ù…","Ø±ÛŒØ§Ø³Øªâ€ŒØ¬Ù…Ù‡ÙˆØ±ÛŒ","Ø§Ù‚ØªØµØ§Ø¯ÛŒ","Ø§Ù†Ù‚Ù„Ø§Ø¨","Ø§Ø®ØªÙ„Ø§Ù_Ø³Ù¾Ø§Ù‡_Ø§Ø±ØªØ´","Ù…Ø¬Ù„Ø³","Ø³Ø®Ù†Ø§Ù†","Ø­Ù…Ø§ÛŒØª","ØªØ¯Ø¨ÛŒØ±","Ø³ÛŒØ§Ø³ÛŒ","Ø³Ø®Ù†Ø±Ø§Ù†ÛŒ","Ø§ØµÙ„Ø§Ø­","Ø§ØµÙˆÙ„Ú¯Ø±Ø§ÛŒØ§Ù†","Ø¸Ø±ÛŒÙ","Ø§Ù‚Ø§ÛŒ","Ø§Ù†ØªØ®Ø§Ø¨Ø§ØªÛŒ","Ù…Ø³Ø¦ÙˆÙ„ÛŒÙ†","Ø¢Ù…Ø±ÛŒÚ©Ø§","Ø´ÙˆØ±Ø§ÛŒ","Ø¬Ù…Ù‡ÙˆØ±ÛŒ","Ø±ÙØ³Ù†Ø¬Ø§Ù†ÛŒ","Ù…Ù„Øª","Ø¨Ø±Ø®Ø±ÙˆØ´ÛŒØ¯Ù‡â€ŒØ§Ù†Ø¯","Ø¯ÙˆÙ„Øª_ØºØ±Ø¨Ú¯Ø±Ø§_Ùˆ_Ø³Ø§Ø²Ø´Ú¯Ø±","Ø±Ù‡Ø¨Ø±ÛŒ","Ø¬Ø§Ù…Ø¹Ù‡","Ú¯ÙØªÙ‡","Ù…Ø·Ø±Ø­","Ø¯Ú©ØªØ±","Ù…Ù†Ø§Ø¸Ø±Ù‡_Ø±ÙˆØ­Ø§Ù†ÛŒ_Ø¨Ø§_Ø®ÙˆØ¯Ø´","Ø§ØµÙ„Ø§Ø­â€ŒØ·Ù„Ø¨","ÙØ±ØµØª_Ø¨ÛŒØ´ØªØ±ÛŒ","Ø§Ø¨Ù‚Ø§ÛŒ_Ø¹Ù„ÛŒ_Ø§ÙˆØ³Ø·_Ù‡Ø§Ø´Ù…ÛŒ"]

url =  baseUrl + "TextSimilarity/GetSyntacticDistance"
payload = u'{"string1": "Ø§ÛŒØ±Ø§Ù†ÛŒ Ù‡Ø§", "string2": "Ø§ÛŒØ±Ø§Ù†ÛŒØ§Ù†", "distanceFunc": 2}'  # JaccardDistance
print(callApi(url, payload, tokenKey))
# result: 0.333333343

url =  baseUrl + "TextSimilarity/GetStatisticalDistance"
payload = u'''{
    "string1": "Ø±ÙˆØ­Ø§Ù†ÛŒ", 
    "string2": "Ø¬Ø³Ù…Ø§Ù†ÛŒ"}'''
print(callApi(url, payload, tokenKey))
# result: 0.905992568

url =  baseUrl + "TextSimilarity/SentenceSimilarityBipartiteMatching"
payload = u'''{
    "string1": "Ø­Ù…Ù„Ù‡ Ù…ØºÙˆÙ„Ù‡Ø§ Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†", 
    "string2": "Ø­Ù…Ù„Ø§Øª Ù…ØºÙˆÙ„Ø§Ù† Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†", 
    "distanceFunc": 2}'''  # JaccardDistance
print(callApi(url, payload, tokenKey))
# result: 0.80357146263122559

url =  baseUrl + "TextSimilarity/SentenceSimilarityWithIntersectionMatching"
payload = u'''{
    "string1": "Ø­Ù…Ù„Ù‡ Ù…ØºÙˆÙ„Ù‡Ø§ Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†", 
    "string2": "Ø­Ù…Ù„Ø§Øª Ù…ØºÙˆÙ„Ø§Ù† Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†", 
    "distanceFunc": 2,
    "minDistThreshold": 0.3}'''  # Ø­Ø¯Ø§Ù‚Ù„ ÙØ§ØµÙ„Ù‡ Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø·Ø¨Ø§Ù‚ (ÛŒÚ©Ø³Ø§Ù† ÙØ±Ø¶ Ù†Ù…ÙˆØ¯Ù†) Ø¢Ù†Ù‡Ø§
print(callApi(url, payload, tokenKey))
# result: 0.75

url =  baseUrl + "TextSimilarity/SentenceSimilarityWithNGramMatching"
payload = u'''{
    "string1": "Ø­Ù…Ù„Ù‡ Ù…ØºÙˆÙ„Ù‡Ø§ Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†", 
    "string2": "Ø­Ù…Ù„Ø§Øª Ù…ØºÙˆÙ„Ø§Ù† Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†", 
    "distanceFunc": 2,
    "minDistThreshold": 0.3}'''  # Ø­Ø¯Ø§Ù‚Ù„ ÙØ§ØµÙ„Ù‡ Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø·Ø¨Ø§Ù‚ (ÛŒÚ©Ø³Ø§Ù† ÙØ±Ø¶ Ù†Ù…ÙˆØ¯Ù†) Ø¢Ù†Ù‡Ø§
print(callApi(url, payload, tokenKey))
# result: 0.714285708963871

url =  baseUrl + "TextSimilarity/SentenceSimilarityWithNearDuplicateDetector"
payload = u'''{
    "string1": "Ø­Ù…Ù„Ù‡ Ù…ØºÙˆÙ„Ù‡Ø§ Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†", 
    "string2": "Ø­Ù…Ù„Ø§Øª Ù…ØºÙˆÙ„Ø§Ù† Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†"}'''
print(callApi(url, payload, tokenKey))
# result: 0.5

################ Call Information Retrieval Function(s) ##################
url =  baseUrl + "InformationRetrieval/KeywordExtraction"
payload = u'''{
    "text": "Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ Ù‡Ù†Ú¯ÙØª Ø§Ù…Ø§Ø±Ø§Øª Ø¯Ø± ØªÙˆØ³Ø¹Ù‡ Ø§Ù†Ø±Ú˜ÛŒ
Ø§Ù…Ø§Ø±Ø§Øª Ø¯Ø± Ù…Ù†Ø§Ø·Ù‚ Ø´Ù…Ø§Ù„ÛŒ Ø§ÛŒÙ† Ú©Ø´ÙˆØ± Ø¨ÛŒØ´ Ø§Ø² Û·Û°Û° Ù…ÛŒÙ„ÛŒÙˆÙ† Ø¯Ø±Ù‡Ù… Ø¯Ø± ØªÙˆØ³Ø¹Ù‡ Ø§Ù†Ø±Ú˜ÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
Ù…Ø­Ù…Ø¯ ØµØ§Ù„Ø­ Ù…Ø¯ÛŒØ±Ú©Ù„ Ø§Ø¯Ø§Ø±Ù‡ Ø¨Ø±Ù‚ Ø§Ù…Ø§Ø±Ø§Øª Ø¯Ø± Ù†Ø´Ø³Øª Ø®Ø¨Ø±ÛŒ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ Ú©Ù‡ Ø§Ù…Ø§Ø±Ø§Øª Ù…ØªØ­Ø¯Ù‡ Ø¹Ø±Ø¨ÛŒ Ø§Ø² Ø§Ù…Ø³Ø§Ù„ ØªØ§ Ø³Ø§Ù„ Û²Û°Û²Û² Ù…ÛŒÙ„Ø§Ø¯ÛŒ Û¸ Ù¾Ø±ÙˆÚ˜Ù‡ Ù…Ù‡Ù… Ø§Ù†Ø±Ú˜ÛŒ Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± ØªÙˆØ³Ø¹Ù‡ Ùˆ Ú¯Ø³ØªØ±Ø´ Ø¨Ø±Ù‚ Ù…Ù†Ø§Ø·Ù‚ Ø´Ù…Ø§Ù„ÛŒ Ø§Ù…Ø§Ø±Ø§Øª Ø§Ø¬Ø±Ø§ Ø®ÙˆØ§Ù‡Ø¯ Ú©Ø±Ø¯.
ÙˆÛŒ Ø§ÙØ²ÙˆØ¯: Ø§ÛŒÙ† Ø·Ø±Ø­â€ŒÙ‡Ø§ Ø¨Ø§ Ù‡Ø²ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø¨Ø§Ù„Øº Ø¨Ø± Û·Û°Û° Ù…ÛŒÙ„ÛŒÙˆÙ† Ø¯Ø±Ù‡Ù… Ø§Ø­Ø¯Ø§Ø« Ùˆ ØªÚ©Ù…ÛŒÙ„ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.", 
    "minWordLength": 3,
    "maxWordCount": 3,
    "minKeywordFrequency": 1,
    "resultKeywordCount": 5,
    "method": "TFIDF"}'''
print(callApi(url, payload, tokenKey))
''' result:
{
    "Ø§Ù…Ø§Ø±Ø§Øª": 75.26119828977285,
    "ØªÙˆØ³Ø¹Ù‡ Ø§Ù†Ø±Ú˜ÛŒ": 23.99621570204974,
    "Û·Û°Û° Ù…ÛŒÙ„ÛŒÙˆÙ† Ø¯Ø±Ù‡Ù…": 21.95849183879027,
    "Ù…Ù†Ø§Ø·Ù‚ Ø´Ù…Ø§Ù„ÛŒ": 20.25414682800825,
    "Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù‡Ù†Ú¯ÙØª Ø§Ù…Ø§Ø±Ø§Øª": 17.670494399999686
}
'''

url =  baseUrl + "InformationRetrieval/StopWordRemoval"
payload = u'''"ØªÛŒÙ… Ù…ØªÙ† Ú©Ø§ÙˆÛŒ ÙØ§Ø±Ø³ÛŒâ€ŒÛŒØ§Ø± Ø¨Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² ÙØ§Ø±Øº Ø§Ù„ØªØ­ØµÛŒÙ„Ø§Ù† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙØŒ ØªØ±Ø¨ÛŒØª Ù…Ø¯Ø±Ø³ Ùˆ ÙØ±Ø¯ÙˆØ³ÛŒ Ù…Ø´Ù‡Ø¯ Ø§Ø² Ø³Ø§Ù„ Û±Û³Û¹Û° Ø¨ØµÙˆØ±Øª ØªØ®ØµØµÛŒ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ ÙØ¹Ø§Ù„ÛŒØª Ø§Ø³Øª. Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û¹Û¶ Ø¯Ø± Ø¬Ù‡Øª ÙØ¹Ø§Ù„ÛŒØª Ù¾Ú˜ÙˆÙ‡Ø´ÛŒ Ø¹Ù…ÛŒÙ‚Â­â€ŒØªØ± Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒØŒ Ø§ÛŒÙ† Ú¯Ø±ÙˆÙ‡ Ø¨Ø§ Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ù…ØªÙ† Ú©Ø§ÙˆÛŒ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø§Ù‡ Ø¹Ù„ÙˆÙ… Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ÛŒØ±Ø§Ù† (Ø§ÛŒØ±Ø§Ù†Ø¯Ø§Ú©) Ù‡Ù…Ú©Ø§Ø±ÛŒ ØªÙ†Ú¯Ø§ØªÙ†Ú¯ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª."'''
print(callApi(url, payload, tokenKey))
# result: ØªÛŒÙ… Ù…ØªÙ† Ú©Ø§ÙˆÛŒ ÙØ§Ø±Ø³ÛŒâ€ŒÛŒØ§Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ ÙØ§Ø±Øº Ø§Ù„ØªØ­ØµÛŒÙ„Ø§Ù† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙØŒ ØªØ±Ø¨ÛŒØª Ù…Ø¯Ø±Ø³ ÙØ±Ø¯ÙˆØ³ÛŒ Ù…Ø´Ù‡Ø¯ Ø³Ø§Ù„ Û±Û³Û¹Û° ØªØ®ØµØµÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ù…Ø´ØºÙˆÙ„ ÙØ¹Ø§Ù„ÛŒØª. Ø³Ø§Ù„ Û±Û³Û¹Û¶ ÙØ¹Ø§Ù„ÛŒØª Ù¾Ú˜ÙˆÙ‡Ø´ÛŒ Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙˆÙ† Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒØŒ Ú¯Ø±ÙˆÙ‡ Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ù…ØªÙ† Ú©Ø§ÙˆÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø§Ù‡ Ø¹Ù„ÙˆÙ… ÙÙ†Ø§ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ÛŒØ±Ø§Ù† (Ø§ÛŒØ±Ø§Ù†Ø¯Ø§Ú©) Ù‡Ù…Ú©Ø§Ø±ÛŒ ØªÙ†Ú¯Ø§ØªÙ†Ú¯ÛŒ.
